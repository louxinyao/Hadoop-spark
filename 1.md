# 手把手大数据——从系统搭建讲起 
本文将从系统环境搭建开始，分别对Hadoop集群的配置，Scala语言，spark编程进行说明，并对可能出现的问题以及本人在搭建过程中产生的一些疑问进行解答。最后，将通过一个电影评分实例来熟悉所学的知识。
本文编写采用Markdown语法，推荐安装插件：Markdown All in One，Markdwon Preview Enhanced，Markdown Image 后进行阅读。

* [一、系统环境搭建](#1)
* [二、系统搭建](#2)
	* [2.1 配置java](#2.1)
* [三、Hadoop集群配置与启动](#3)
	* [3.1 集群配置的准备](#3.1)
	* [3.2 ssh免密设置](#3.2)
	* [3.3 所有节点都关闭安全策略](#3.3)
	* [3.4 集群配置文件](#3.4)
	* [3.5 配置文件分发](#3.5)
	* [3.6 使用脚本分发](#3.6)
	* [3.7 hdfs命令](#3.7)
	* [3.8 运行MapReduce程序](#3.8)
	* [3.9 hdfs](#3.9)
	* [3.10 Hadoop集群的伸缩](#3.10)
* [四、scala语言及环境启动](#4)
	* [4.1 Scala原生环境](#4.1)
	* [4.2 scala集成开发环境](#4.2)
	* [4.3 scala语言入门](#4.3)
	* [4.4 scala语言的函数式编程](#4.4)
* [五、spark编程](#5)
	* [5.1 启动spark的scala环境](#5.1)
	* [5.2 RDD相关概念](#5.2)
	* [5.3 spark程序运行模式](#5.3)
	* [5.4 spark集成开发环境的使用（idea）](#5.4)
	* [5.5 RDD编程](#5.5)
	* [5.6 RDD操作](#5.6)
	* [5.7 分区](#5.7)
	* [5.8 生成jar包送到集群运行](#5.8)
* [六、电影数据实例分析](#6)
	* [6.1 数据集的数据格式](#6.1)
	* [6.2 找出评分排名前十的电影名](#6.2)
* [七、后记](#7)


## 一、系统环境搭建 <a id ="1"></a>
### 1.1 操作系统：Linux 
所选用的操作系统为 Linux
选择Ubuntu发行版22.04.2LTS
官网下载地址：https://ubuntu.com/download/desktop
下载建议：不要选择最新版的，不一定稳定。

>Q：为什么选择Ubuntu
A：开源、不收费。


### 1.2 在virtualBox中打开Linux 
#### 1.2.1 打开virtualBox
右键-以管理员身份运行
#### 1.2.2 创建虚拟机
主页面-新建 类型：Linux 版本Ubuntu-64bit
![图 1](images/a1cfef26557316f52bd700293541bad4bfe4c20ac1f22cb8fa0a27a7c215a906.png)  
！！注意 这一步要选动态分配
![图 2](images/3618f5645e70722cddc8e4be1d0fafb1b05bfc6e438ffb5a453007b38fd5d032.png)  
因为这样对磁盘空间的利用率比较高，但启动前要检查磁盘剩余空间大小，太小容易宕机（（
建议虚拟磁盘大小>=60G

### 1.3 设置虚拟机硬件
主界面-设置
系统：CPU核心数>=2 内存>=4G（这里我分别选择了3 8G）
网络：台式机选择桥接网卡
光驱：选择下载的光驱
### 1.4 启动
按照提示进行操作即可

### 1.5 在VirtualBox中打开虚拟机映像
在主页面点击注册，按提示打开之前配置好的文件（好像直接从之前的文件夹里直接进去也可以）
#### 1.5.1 启动前配置
* 查看CPU 内存 
* 网络设置：连接方式：桥接（当然如果是笔记本的话网络地址转换也没啥问题）
*  MAC地址（！！！每次最好刷新一下… 因为如果mac地址相同是一件很麻烦的事情…可能就上不了网了）

### 1.6 配置Linux
#### 1.6.1 连接网络
左上角点击这个图标
![图 1](images/e4af65e219fc74533ca659e23a97a8cf8444d03771ba3e31e7ca6b7ae64d398b.png)  
按照下面内容进行设置（这里的.103是我自己用的）
![图 2](images/550cc4496d00d36c31cb7dd613ca149982a9dc0463e27bfc6067b5c3c31a64ce.png)  

总之就是按照网管给你的ip地址和网络进行配置就可以。配置好之后记得上一下网看看网络是不是能正常使用了。

#### 1.6.2 安装虚拟机增强功能
目的是为了虚拟机显示的尺寸能够自适应，并且与宿主机的粘贴板、文件夹共享。
**tips：在Linux中，没有扩展名的概念，一般.sh结尾的文件就是可执行文件。在今后，我们会遇到很多次这种情况。**

**操作步骤如下：**
点击菜单栏 “设备”->“安装增强功能”
这时候会发现虚拟机页面上会多一个光盘的图标，点击打开；
会发现里面有一个autorun.sh文件，运用这个文件进行安装，但是这个文件打开是类似.txt的文本文件不能直接运行，我们应该打开一个终端（terminal），在终端中执行 ./autorun.sh。
最后根据提示，回车退出窗口后重启虚拟机即可
（好像会跳出一个对话框问你是否要restart，选择restart即可）

重启完成后，点击菜单栏中的自动调整显示尺寸即可。同时，设置共享粘贴板等（双向）。

#### 1.6.3 配置软件更新
点击左下角
![图 3](images/6e1c4b7226136b0a231749e4446166c97ca1b072dcd2ab5c1eb1b62323913362.png)  
![![图 3](images/6e1c4b7226136b0a231749e4446166c97ca1b072dcd2ab5c1eb1b62323913362.png)  
 1](images/099962ce785afcca66375109ff0b1da5ce53a3dfa00fd9910b8509ac9257d998.png)  
如果显示“checking for updates”，点击“stop”，然后就会跳出下面这个对话框：
![图 5](images/a65169e0857a4b46dddf54ef78afe8e3d6b1ae7ea3bbd5c1adce2307f13a17da.png)  
点击“settings”
![图 7](images/3df8ce337d6fa886f2164df38fb95393269b08921e7d77fd19f5942feea7c2f0.png)  
在这里对下载源进行选择，杭州一般选aliyun或huaweiCloud。其他地区自己选一个比较快的就行。然后直接close所有出现的窗口
进行软件源初始化：在终端执行：sudo apt-get update
如果要全部更新，在终端执行：sudo apt-get upgrade
#### 1.6.3 ssh服务安装
ssh是采用非对称秘钥的远程登录工具.
在之后配置好集群后。我们可以采用这种方式登录其他节点：
ssh slave0
但是在没有进行免密设置的情况下，需要每次都输入密码。所以在之后我们会进行ssh免密设置，更加方便。[点击跳转阅读3.2 ssh免密设置](#ssh)
ssh服务包括两个：client，server
默认安装好了client，现在安装server：sudo apt install openssh-server
#### 1.6.4 vim安装与使用
安装:sudo apt install vim
vim有三种模式：命令模式 编辑模式 底行命令模式
三种模式的切换：
刚进入时是命令模式，按i进入编辑模式，按Esc回到命令模式
在命令模式下，按":"进入底行命令模式，按Esc回到命令模式

### 1.7 系统设置 <a id ="host"></a>
#### 1.7.1 修改计算机名 
计算机名在 /etc/hostname
编辑hostname：sudo vim /etc/hostname 

将文件内容改成想要的计算机名 master/slave0/slave1
这样在之后远程登录的时候就可以直接写 ssh master之类的，就很方便
#### 1.7.2 配置域名解析文件
域名解析文件：/etc/hosts
命令：sudo vim /etc/hosts
>加sudo是因为修改计算机名需要高级权限，不加会出现这种错误：

![图 2](images/06fc3f63329d3c944a58f564f1aaa49a4c6df10bc22417725cb3ebb3b328fef9.png)  

出现如下界面：
![图 1](images/57cc67a580e1675306be68d49417a64fcd557450bc3731e0528dfb4813ee9275.png)  
#### 1.7.3 软件版本选择
从spark->Hadoop->Java顺序选择
从spark.apache.org看：
![图 1](images/cba9fd94b3c9eee2386e5e3e6fec9acc39fea7ada5c9076990697b754dbf4f85.png)  
下载二进制包，所以spark选择稳定版3.2.3，Hadoop选择3.2.
从apache.hadoop看：
java版本选择：
![图 2](images/588eb2cdde9129e13f50b459fc089fdcdadc5e794f804522f4589a685dd8df62.png)  
java选择1.8.0_315

## 二、配置Hadoop单机版（StandAlone） <a id ="2"></a>
软件安装方式：（1）系统安装；（2）下载，配置路径
我们使用第二种方式，优点是对系统没有影响，其他用户不可见，缺点是其他用户不可见
### 2.1 配置java <a id ="2.1"></a>
**下载并解压缩java:**
解压缩命令：tar -zxvf 文件名（tar -zxvf jdk-8u351-linux-x64.tar.gz ）
验证软件：
![图 3](images/af2dd2cf7340b1760eb2c13b490fd21592d92e0faa63fb8acb0e89cb60ac4c04.png)  

### 2.1.1 配置java环境变量

>配置JAVA_HOME：<a id ="homePath"></a>
配置原因：
有一些基于 Java 开发的工具会用到JDK的路径，所以我们配置JDK的路径给JAVA_HOME。

>配置CLASSPATH：
配置原因：
CLASSPATH顾名思义为包路径，告诉Java在执行的时候，去哪里找到需要的包和类供程序使用。所以配置时应把包的路径赋值给CLASSPATH。
（以上解释来源：https://zhuanlan.zhihu.com/p/153500777）

路径位置在文件.bashrc中。
编辑.bashrc：vim .bashrc，添加
``` shell
export JAVA_HOME=~/software/jdk1.8.0_351
export PATH=$JAVA_HOME/bin:$PATH
```
![图 4](images/479653efcfd0d9691255d08b94e773d868b7d12a7cdc257e650ebc7fd7755aed.png)  
新打开一个终端验证：
![图 6](images/5325ac9d8907a1a819a460e670c8228e51b2b5eaa30c8d4ce3f6cf41e9760fd7.png)  
出现上图即可认为配置成功

### 2.1.2 配置Hadoop
**下载并解压缩:**
tar -zxvf hadoop-3.2.2.tar.gz 
验证：
找到Hadoop命令，执行：./hadoop
![图 8](images/e4bf2a6798574622196d6896caf4531f9f6c1f934d6f38414d66e3fa0e6e7b9e.png)  
出现上图即可认为配置成功
和之前的道理一样，我们也需要配置有关的环境变量。

配置路径：
编辑.bashrc：vim .bashrc，添加：
``` shell
export HADOOP_HOME=~/software/hadoop-3.2.2
export PATH=$HADOOP_HOME/sbin:$PATH
```
验证：
![图 9](images/f9d1305c4bf096b03fe2c4e307822075892b5806e16fe9dad429e599b2b21dde.png)  

配置 HADOOP-env.sh：vim hadoop-env.sh
![图 10](images/77a9bee86b603b96ec3f8cb5265b6dea3c47f6996021f4c562e6c63fbb49fd16.png)  

在最后添加
``` shell
export JAVA_HOME=~/software/jdk1.8.0_351
```

>其实在这配置的时候我一直有个问题，就是我们明明前不久刚配置过JAVA_HOME，为什么还要在hadoop-env.sh中再次配置？为什么系统默认的JAVA_HOME读不到？查阅了很多资料终于找到一个合理的解释：“hadoop执行启动脚本时候会加载这个hadoop-env.sh，如果这里面配置了JAVA_HOME会覆盖/etc/profile里面的$JAVA_HOME 先启动NameNode，然后再通过ssh启动slaves（也就是DataNode、NodeManager这些） ssh过去的时候如果配在/etc/profile，每次都得source一下~ 所以，Hadoop放在hadoop-env.sh里面，这样的话ssh远程执行的时候也不用source /etc/profile了。”
此外，借助Hadoop-env.sh文件中的原文注释 Set Hadoop-specific environment variables here. The only required environment variable is JAVA_HOME. All others are optional. When running a distributed configuration it is best toset JAVA_HOME in this file, so that it is correctly defined onremote nodes. 翻译过来大概是： 在这里设置特定于hadoop的环境变量。 唯一需要的环境变量是JAVA_HOME。其他都是可选的。在运行分布式配置时，最好在这个文件中设置JAVA_HOME，以便在远程节点上正确地定义它。
总结一下就是远程登录时的环境变量的问题。
（以上解释来源：https://bbs.csdn.net/topics/398691342 及 hadoop-env.sh官方注释）

### 2.1.3 运行Hadoop-example
运行grep：查找目录下所有文件中符合条件的内容
建立输入目录，并将要查找的文件放入目录中
```shell
mkdir testHadoopExample #在家目录下新建一个测试目录
cd testHadoopExample
mkdir input1
cp $HADOOP_HOME/etc/hadoop/*.xml input1 #拷贝文件到input1目录下
hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep 
input1 output 'dfs[a-z.]+' #运行例程
cd output
cat part-r-00000 
```
![图 1](images/9736317b4dde357afb11df77f249631034b482afa58e87a74ea921d20489f873.png)  
![图 2](images/2840d89ab59a9c9cd18f1f06085e58bc1f43aa579983dfe69bf674eef1ddfcc8.png)  
![图 3](images/c3f0cdc363d4ffaac5ed6f6ac33d2197770ec5923dbf507cf6d28b1429a6b075.png)  
![图 4](images/76736d8e1f1d49ec40684740e543846dbbe159ce01fdafe184af753dab690b6c.png)  


## 三、Hadoop集群配置与启动 <a id ="3"></a>
完成单机版的配置后，同样的，我们就能很快完成其他节点的配置，从而为搭建集群做好准备。
### 3.1 集群配置的准备 <a id ="3.1"></a>
#### 3.1.1 已经能够在单机模式下运行
在所有节点上都有Hadoop软件及配置，都能够在单机模式下运行
#### 3.1.2 节点准备
准备三个Linux节点，节点之间网络通讯正常，ssh相互登录正常
>ssh连接命令格式：ssh 用户名@ip地址
连接方式：linux、mac可以直接连接，windows可以用wsl或第三方ssh工具。

所用的三个Linux用户名要相同
修改主机名：master slave0 slave1
配置所有节点的域名hosts
(注意，每个节点上都要配置！(踩过坑的本人狠狠落泪))
比如我的hosts：
![图 1](images/a3083e41918f150552d221b5fde9b0ced7126d2089bfef1a6cda3e952f08cc47.png)  
>这一步不会的移步：[点击跳转阅读 1.7 系统设置](#host)

>这里补充一下 NameNode DataNode：
NameNode节点：存储所有hdfs文件的名字、属性、存储位置、校验值等。一般用master节点
SecondaryNameNode：第二NameNode，跟NameNode同时运行，但不与NameNode完全同步，可能差一个心跳
DataNode节点：存储文件内容（可能多份 分块   一般用slave节点

### 3.2 ssh免密设置 <a id ="3.2"></a>
#### 3.2.1 检查ssh连通性 <a id ="ssh"></a>
从master节点开始对master slave0 slave 
**分别执行**
ssh master 
ssh slave0 
ssh slave1
>（又是一个分别执行！！此时又需要大家在多台电脑间移动了…但是没关系…再过一会学完下一节就不会这么痛苦了…） 

如果连通不了，一般是这两种情况：
* 网络不通，这就没啥好说的了只能乖乖回去检查网络配置= =
* 软件没有安装，使用如下命令安装：
&emsp;&emsp;sudo apt install openssh-client
&emsp;&emsp;sudo apt install openssh-server

#### 3.2.2 ssh 免密设置
过程：
生成密钥对
传送公钥到其他节点

从master开始：
生成密钥对，命令：ssh-keygen -t rsa 
一路回车到底 如果提示覆盖，选择yes
传送公钥到其他节点，命令为：
ssh-copy-id master
ssh-copy-id slave0 
ssh-copy-id slave1

**注意，这里也需要每个节点分别执行生成传送操作，且密钥对需要传送给自己！**

验证，从master开始，分别执行：
ssh master 
ssh slave0 
ssh slave1 
都不需要密码。

如果你某一步不小心出了问题，但是又不知道怎么解决…那就移除密钥后再来一次：
移除密钥： rm -r .ssh
然后再按照上面的步骤再来一次就可以。

（恭喜大家！操作到这里之后总算不用再在多台电脑直接来回跳跃了！！直接ssh就可以免密登录其他电脑了~）

### 3.3 所有节点都关闭安全策略 <a id ="3.3"></a>
Linux一般有防火墙，selinux
Ubuntu22.04的desktop版本防火墙默认没有打开，selinux没有安装
验证防火墙状态：sudo ufw status
![图 1](images/fc4c3569c9c112e6feac29141a27474c2a04085a23f1e0d94d43748686ae0138.png)  

### 3.4 集群配置文件 <a id ="3.4"></a>
去hadoop官网上查看官方文档，我们得知：
默认配置文件（只读）：core-default.xml, hdfs-default.xml, yarn-default.xml and mapred-default.xml.
需要配置的文件：core-site.xml, hdfs-site.xml, yarn-site.xml and mapred-site.xml.
在配置文件中配置的选项将覆盖默认配置文件中的对应选项
默认Hadoop on yarn
#### 3.4.1 core-site.xml
在core-default.xml中的设置值是<value>file:///</value>  指向本地系统文件的根目录，
由于我们当前要使用hdfs文件系统，所以改为：
``` xml
<property>
  <name>fs.defaultFS</name>
  <value>hdfs://master:9000</value>
</property>
```
表达的是：默认文件系统协议是hdfs://，根目录节点是master:9000

为了提高大文件的读取速度将io.file.buffer.size设置为128k
```xml
<property>
  <name>io.file.buffer.size</name>
  <value>131072</value>
</property>
```
配置完成后：
![图 2](images/3fc5ee1de1de3440adf7cd9db94a864029a88fe4dd52ed3b569ae1d06217995b.png)  

#### 3.4.2 hdfs-site.xml
查看
![图 3](images/52e3379e4a8d6c25d3a830b2bd89e8fd91f73cc70b8afe292e0ca20dffcc38c0.png)  
存储位置中有${hadoop.tmp.dir}
hadoop.tmp.dir在core-site.xml有设置 值为 
![图 6](images/41637d345b871a9f22583f4589335f349d44c79ee10a61fa70a44b683981b58c.png)  

由于不在家目录下，有可能存在读取文件问题，所以在core-site.xml中重新设置
```xml
<property>
  <name>hadoop.tmp.dir</name>
  <value>/home/bigdata3/hadooptmpdir</value>
</property>
```
设置完成后，为
![图 7](images/310f4de5665edfa01c2f101d2950d77d1ac8df76495188c8b5320879390cd4ad.png)  

结果：hdfs-site.xml没有添加设置

#### 3.4.3 yarn-site.xml

yarn.resourcemanager.hostname
yarn.nodemanager.log-dirs
yarn.nodemanager.remote-app-log-dir
yarn.nodemanager.aux-services

设置为
```xml
<configuration>

  <property>
    <description>The hostname of the RM.</description>
    <name>yarn.resourcemanager.hostname</name>
    <value>master</value>
  </property>    
   <property>
    <name>yarn.nodemanager.log-dirs</name>
    <value>/home/bigdata3/hadooptmpdir/logs/userlogs</value>
  </property>
<property>
    <description>Where to aggregate logs to.</description>
    <name>yarn.nodemanager.remote-app-log-dir</name>
    <value>/home/bigdata3/hadooptmpdir/logs/logs</value>
  </property>
  <property>
    <description>A comma separated list of services where service name should only
      contain a-zA-Z0-9_ and can not start with numbers</description>
    <name>yarn.nodemanager.aux-services</name>
    <value>mapreduce_shuffle</value>
    <!--<value>mapreduce_shuffle</value>-->
  </property>
</configuration>
```
![图 8](images/0fbe9c384f8218cb6b8bafd7472f7a1e2b7c6d50dbbe396a26b4e7640189022a.png)  

#### 3.4.4 mapred-site.xml
```xml
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>
<property>
  <name>yarn.app.mapreduce.am.staging-dir</name>
  <value>//home/bigdata3/hadooptmpdir/hadoop-yarn/staging</value>
</property>

```
![图 2](images/12427587ea4d5639613a8abd7891673b37c17d796160fa47d97c09a2de9edb30.png)  

#### 3.4.5 workers 
workers文件中写入所有的DataNode节点域名（slave0 slave1） 一行一个
![图 1](images/bd70359cf0f4d75977f972c5291dfa9b867feafdc8ae7ce56ee6b5039818968d.png)  

（先从software/hadoop-3.2.2/etc/hadoop中将workers拷贝到之前的hadoopconfigTmp/hadoop中）

#### 3.4.6 文件夹的处理
在hadoop-env.sh中将需要的文件夹修改位置为：
``` shell
export HADOOP_PID_DIR=/home/bigdata3/hadooptmpdir/HADOOP_PID_DIR
export HADOOP_LOG_DIR=/home/bigdata3/hadooptmpdir/HADOOP_LOG_DIR
```
（同样也要把文件先拷贝过去）
![图 2](images/da0f94173e89e719bbdc18f3ffb0c346b5d74580ae5d583e03a310f268a23642.png)  

### 3.5 配置文件分发 <a id ="3.5"></a>
当前目录在配置文件编辑目录下：/home/bigdata3/hadoopConfigTmp/hadoop
本地配置文件应该所在位置：/home/bigdata3/software/hadoop-3.2.2/etc/hadoop

>(什么意思呢…就是当时为了防止乱修改导致原来的都崩了，于是把原始文件copy到了一个新文件夹底下先改改看看，现在ok了当然要返回修改原始文件)

远程配置文件目录应该与主机相同。
A) 如果远程机器已经有了相应的软件包（Java Hadoop）

&emsp;&emsp;本地分发：
&emsp;&emsp;cp * /home/bigdata3/software/hadoop-3.2.2/etc/hadoop

&emsp;&emsp;远程分发：
&emsp;&emsp;scp * slave0:/home/bigdata3/software/hadoop-3.2.2/etc/hadoop
&emsp;&emsp;scp * slave1:/home/bigdata3/software/hadoop-3.2.2/etc/hadoop

![图 3](images/0edc1f8fde389ab2780a41f23cfbd5bd2bc31dfcf1a4f7d64f156e65a8bc4e60.png)  

&emsp;&emsp;系统文件分发：
&emsp;&emsp;回到主机的家目录下，分发.bashrc
&emsp;&emsp;scp .bashrc slave0:~
&emsp;&emsp;scp .bashrc slace1:~

![图 4](images/51e17ddf9426ced21c1aa8059b46cfe94476e4f15d189af321aa8aa850ddbfa4.png)  


B) 如果远程机器没有相应的软件包（Java Hadoop）

&emsp;&emsp;本地分发：
&emsp;&emsp;cp * /home/bigdata3/software/hadoop-3.2.2/etc/hadoop

&emsp;&emsp;远程分发：回到家目录
&emsp;&emsp;scp -r software slave0:~
&emsp;&emsp;scp -r software slave1:~

&emsp;&emsp;系统文件分发：
&emsp;&emsp;回到主机的家目录下，分发.bashrc
&emsp;&emsp;scp .bashrc slave0:~
&emsp;&emsp;scp .bashrc slave1:~

### 3.6 使用脚本分发 <a id ="3.6"></a>
>我们在搭建Hadoop完全分布式集群的过程中，要修改很多配置文件或者是安装很多软件，在这个过程中，如果集群数量很大，我们不能把每个节点都编辑一遍，这样既费时又容易出错。因此只要在一个节点编辑好，再通过分发脚本发送给其他节点就可以了。

把上面的内容整合成一个可执行文件，每次调用时只需要调用这个文件即可实现分发。
#### 3.6.1 集群的启动
##### NameNode格式化 
配置好以后，第一次启动集群之前，需要进行hdfs格式化，格式化只在NameNode上。（只要格式化一次！！）
hdfs namenode -format
注意：
* 如果原来hdfs系统有数据，格式化将会破坏数据
* 如果原来hdfs系统有数据，格式化后，启动集群将进入安全模式。所以在格式化之前要删除数据：删除所有节点上的数据文件夹
![图 5](images/145258966d5b779576a34b00040a684692066f0009e149b131b8e2aa1664e54b.png)  

如果不小心进入了安全模式，那么在安全模式下输入指令：
hadoop dfsadmin -safemode leave
即可退出安全模式。

#### 3.6.2 启动集群与关闭集群
使用脚本启动：start-all.sh
使用脚本停止：stop-all.sh
按照当前配置，启动停止都应该在master节点上。

启动后检查：
（1）用jps
按照当前配置，NameNode上
![图 6](images/9185b537fc5b10b29033f8aa4a197d22f2fbd7f6c2f00e672fcc217cde0b3217.png)  
datanode上：
![图 7](images/7d4850ef92c2e83a606d2691c9187710508e0e0a412183b86fa47d73c5f8b944.png)  

（2）用web页面
地址：master：9870
![图 8](images/3707b272bef7ab5d2ac6fb48e0493ea22ca3f071864024b9bf6178ca0666a9c6.png)  


（3）上传文件测试
命令格式 hadoop f -put 文件名 hdfs路径
例如：传test.txt到hdfs根目录
hadoop fs -put test.txt/

![图 9](images/8c0cec8394de45417d40235ea21c5576fefc30db5129a5f20d6d6c3fcad28ac8.png)  

上述都没问题的话，就说明集群启动成功了。

#### 3.6.3 配置文件分发脚本制作
Windows下可执行文件有.exe .bat .cmd
Linux下没有具体的可执行文件的规定，所有文件都可以是可执行文件，只要赋予可执行权限
##### 可执行文件的建立
* 文件内容要是shell可以解释的命令
* 执行时要全路径执行，或者在系统变量PATH里添加路径
* 要有可执行权限，如果没有可执行权限，可以将文件作为参数传给bash执行
* 赋给可执行权限用chmod命令，形式： chmod +x 文件名
（要是chmod+x还是提示你没权限的话…那就chmod 777赋予所有权限）

新建一个transConfig文件测试，文件中内容为：echo hello。（运行结果为打印hello）
利用上述两种方式分别执行：
![图 1](images/cfb878a056a002e7e455412afe83f1fdd55455c9e8eef7ce7845547693d87bdb.png)  

![图 2](images/1a4c336fdc057a4e816c61207de4c7ccc0976f796664ee2a0d22297c25a18d8b.png)  
说明没问题，下面开始编写正式的分发脚本。
放在myShellScripts文件夹下,一般固定路径
在.bashrc文件中新添加PATH，这样系统才知道有这个。
```shell
export PATH=/home/bigdata3/myShellScript:$PATH
```
添加文件内容为：
```shell
#echo hello
configPath=/home/bigdata3/hadoopConfigTmp/hadoop
hadoopConfigPath=/home/bigdata3/software/hadoop-3.2.2/etc/hadoop
cp $configPath/* $hadoopConfigPath
scp $configPath/* slave0:$hadoopConfigPath
scp $configPath/* slave1:$hadoopConfigPath
scp ~/.bashrc slave0:~
scp ~/.bashrc slace1:~
```
执行：
![image.png 1](images/7af8f951a4aa907d055a1a51ab0cce4499f6a6cafe136aa8c297b1b86d638b3f.png)  

最后，重启集群使配置生效。

### 3.7 hdfs命令 <a id ="3.7"></a>
#### 1）创建目录 
hadoop fs -mkdir /路径/目录名
#### 2）拷贝文件到hdfs 
##### （1）从本地文件拷贝到hdfs，如果确定要覆盖同名文件，加参数-f。
hadoop fs -put 本地源文件 hdfs路径/文件名（如果不写文件名则使用原来的文件名）
（从本地文件找）
hadoop fs -put 与 hadoop fs -copyFromLocal相同

##### （2）从hdfs拷贝到hdfs，如果确定要覆盖同名文件，加参数-f。 
hadoop fs -cp /路径/源文件 /路径/目标文件      
（默认源文件从hdfs上找，比如）
![图 1](images/6d3732ffcbe94e7c27ccbd6e49e04c9ebcb90ef8db1e137cb86f9f8253a45c2f.png)  
上图这样写 会提示找不到 这是因为hdfs上没上传
执行下面命令
![图 2](images/4abb4abb665ef2f74b9cfb919d3fb8add49e22c3cb01b821ab0eecebba658288.png)  
成功后，在web页面也可以看到
![图 3](images/fcc7b128292661ec53911a2fc844978a19d9db4f9e1f5ad2f161915232df17e6.png)  

#### 3）从hdfs拷贝到本地 
hadoop fs -get hdfs路径/文件名 本地路径/文件名 （如果不写文件名，则默认使用原来的文件名）
copyToLocal与get相同。
#### 4）直接显示hdfs上的文件内容 
hadoop fs -cat hdfs路径/文件名
![图 4](images/befbc5a7b55e3c13a53f298993ecf689c5e97936dd7aab5d98d742b9cd4fe022.png)  
#### 5）显示hdfs对应目录下的内容 
hadoop fs -ls hdfs路径
#### 6）移动文件 
-moveFromLocal，-moveToLocal，用法同-put和-get
-mv用法同-cp
#### 7）删除文件 
hadoop fs -rm hdfs路径/文件名
#### 8）删除目录 
##### （1）删除空目录 
hadoop fs -rmdir hdfs路径/目录名
注意 只能删除空文件夹。优点：安全性好 缺点：比较麻烦
![图 5](images/a4be17ea300b54ec5e431b484139c4af15f0bce4e015d24c45d0f4d85cd2d7bb.png)  
##### （2）删除非空目录 
hadoop fs -rm -r|-R hdfs路径/目录名
![图 7](images/a09b182119601f0ae87d7b0d01784dd517797cce8742100f202d36d6a7e5881b.png)  

![图 6](images/f7ee2eacff8b70641d1fde44dd249fce6ec86382c65774db5073b4a31c823404.png)  
#### 9）在hdfs上新建一个文件 
hadoop fs -touch hdfs路径/文件名
例如
hadoop fs -touch /hdfs_a.txt
![图 8](images/4179e2c6f2cdb4edec39dea0a48452326cf024f998b391d55df1e3128f5d98df.png)  
#### 10）向hdfs文件写入内容 
hadoop fs -appendToFile 本地源文件 hdfs路径/文件名
![图 9](images/fb696e271acf0f0584a8163866d2b144743b4b1edf55089017dfdb04facefb0d.png)  


### 3.8 运行MapReduce程序 <a id ="3.8"></a>
>通过3.6中的操作，我们只能确定hdfs没有问题，即文件存储没有问题，但是计算是否存在问题无法确定。因此，需要执行程序，看看计算是否有问题。如果有，则需要进行修改。

运行程序为：
运行hadoop-mapreduce-examples-3.2.2.jar grep。

##### （1）在hdfs上创建input目录 
&emsp;&emsp;hadoop fs -mkdir /input
##### （2）上传input操作文件到/input目录 
&emsp;&emsp;hadoop fs -put $HADOOP_HOME/etc/hadoop/*.xml /input
##### （3） 程序执行 
&emsp;&emsp;hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep /input /output 'dfs[a-z.]+'

>为什么加了/就表示分布式文件系统了呢？因为hadoop中执行的时候，会查看配置。在配置core-site的时候已经默认文件系统为master:9000,也就是分布式文件系统的命令节点。

执行会出错，按提示修改配置mapred-site.xml
新增内容
```xml
<property>
<name>mapreduce.map.env</name>
<value>HADOOP MAPRED HOME=${HADOOP HOME}</value>
</property>
<property>
<name>mapreduce.reduce.env</name>
<value>HADOOP MAPRED HOME=${HADOOP HOME}</value
</property>
```
 分发配置，执行 transConfig.sh
 然后再次执行hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar grep /input /output 'dfs[a-z.]+'

 成功的话应该长这样
 ![图 1](images/1f672f46d7b52c64240103343dd332ed5748cb9d8165139b3668786f56ef93dc.png)  
##### （4）查看执行结果 
&emsp;&emsp;hadoopfs -cat /output/part-r-00000
![图 3](images/ce52add29494160d887dab03d061764112bf09533adb5169e7f60db49e20b5eb.png)  

### 3.9 hdfs <a id ="3.9"></a>
#### 3.9.1 replication
replication：指文件的副本数，就是同样的文件存了几份
replication通过hdfs-site.xml进行配置,默认是3，在hdfs-default.xml中
```xml
<property>
  <name>dfs.replication</name>
  <value>3</value>
  <description>Default block replication. 
  The actual number of replications can be specified when the file is created.
  The default is used if replication is not specified in create time.
  </description>
</property>
```

但是，实际副本数与节点数有关。如果副本数设置>slave的节点数，则实际副本数是slave的节点数。

#### 3.9.2 文件存储路径
在hdfs的web页面上，查看id和节点位置：
![图 1](images/ac6eeff16bbabe633cd7fc93c873d232670370b67badcda36b56e56ce9c965cd.png)  
文件存储的根路径是我们在core-site.xml中设置的
![图 2](images/655ac6e896c1ca1dd00a83aeed3374565664679d9a4da4585cc5e67d730c4ba5.png)  
查找到相应的Block Pool路径
![图 3](images/a0b1230e98bbf0a7acd09e28f5e0dcf6666f93d70d1c8aeb5c890877e7b7a7f9.png)  
在 Block Pool中找到blk_1073741857
![图 4](images/c34be3fac56ec4a0ea8045f21b46f73a8a22d38dd40ef2320ce4019c0c6247c6.png)  
查看文件内容
![图 5](images/6e1a0f7c0c411f87df3344e0994edbb152f6403927a76c8ce0dc9e056c60de4d.png)  

在slave0上也进行同样的做法，发现是一样的：
![图 6](images/946448042b94e3f25a9cd57782d9a2a5916e21dee175b3308bc7f5d9dccdbeb1.png)  

#### 3.9.3 hdfs的BlockSize
块大小。在大数据背景下，文件都非常大，为了提高处理速度，所以块大小（默认是128M）远大于本地文件的块大小（一般默认是4K）
大数据的文件一般都采用追加方式，key：value
修改通过hdfs-site.xml 的配置，默认配置是128M，在hdfs-default.xml中，修改为64M
```xml
<property>
  <name>dfs.blocksize</name>
  <value>67108864</value>
</property>
```
上传一个大于64M的文件，查看存储形式，可以看到文件分3块存储：
![图 1](images/cdca782c3320bff1f8020c7887303d1a51efef51c5c02aee5c0650e0f544a398.png)  
![图 2](images/b90f986cf7d25b1515880f3fd3d10f6290ac346239a953bb0bd44557e8a6e539.png)  
![图 3](images/4949915124cf52b5520fece04f076d072bb68b8931ebd3aff840e2bd90accba6.png)  
也就是说，当上传的文件>块大小时，会进行切块，然后分块存储。

### 3.10 Hadoop集群的伸缩 <a id ="3.10"></a>
#### 3.10.1 Hadoop集群的扩张
* 准备一个安装了Linux的节点，配置好网络
* 修改主机名为集群中所使用的名称
* 添加域名：修改集群中所有节点的hosts文件，将新节点加入，集群所有节点的有关集群的域名部分要相同
* 免密设置：原集群中所有节点都将公钥（ssh-copy-id）传送到新节点，新节点产生密钥对，并将公钥传送到所有节点，包括自己
* 修改分发文件，分发软件到新节点
    分发软件： scp -r /home/bigdata3/software slave2:~
* 修改集群配置配置，将新节点加入workers文件中：scp ~/.bashrc slave2:~
* 修改配置分发文件，将新节点加入
* 分发配置文件
* 重启集群：先用stop-all.sh关闭集群，再用start-all.sh打开集群
* 注意：不要格式化hdfs 
#### 3.10.2 Hadoop集群的收缩
##### （1）简单粗暴的方法
将要切除的节点直接关机或从网络上去除。注意：需要一台一台的进行，不可以同时关闭多台。
##### （2）修改配置文件workers
将需要切除的节点从workers中删除，然后分发配置，重启集群。

## 四、scala语言及环境启动 <a id ="4"></a>
### 4.1 Scala原生环境 <a id ="4.1"></a>
安装Scala环境：下载、解压、配置路径。
解压缩：tar -zxvf scala-2.12.15.tgz
测试：进入scala目录下的/bin
![图 1](images/ac3a1e8094d9f29076dd9f6a1f2e6dd543628e1c291311ecd095eb09230c20d7.png)  
每次如果都要这样进入scala比较麻烦，因此进行路径配置。
配置：
* 赋值SCALA_HOME
* 将scala命令的路径配置到PATH
在家目录下，编辑.bashrc，添加：
```shell
export SCALA_HOME=/home/bigdata3/software/scala-2.12.15

export PATH=$SCALA_HOME/bin:$PATH
```
保存退出。

测试：新启动一个终端，直接输入scala
![图 2](images/6dbbe36935b9ed30359e050a9de997e7e130aaa6caf08f0fcea545a62cb13c2b.png)  

### 4.2 scala集成开发环境 <a id ="4.2"></a>
>由于一直在黑框框里写实在是不太友好，所以先装个idea，然后再idea里写scala。

安装idea集成开发环境：下载、解压、配置路径。
解压缩：tar -zxvf ideaIC-2023.1.tar.gz
测试：进入idea目录/bin，执行：./idea.sh
配置路径：编辑.bashrc，加入：
```shell
export PATH=/home/bigdata3/software/idea-IC/bin:$PATH
```
>只引入PATH是因为不需要HOME，具体原因移步：[点击跳转阅读 配置HOME与PATH的原因](#homePath)

保存退出后，新打开一个终端，直接输入idea.sh，
![图 1](images/c1d9a26550783b6ac3de7a1decf44c3126d1901257ceafe4e8cce18ce337a22e.png)  

同意协议后：
![图 2](images/357ef32f13b6f41f7dca64a8bc1c21f09ce71a90436e618e837526821009e4c0.png)  
点击“New Project”
![图 4](images/1f2a6e9a52ecfa48bb6864464689b78277636c0eb74bfcbe1b24f89ef23a6f6e.png)  

在language处点击加号，选Scala
![图 3](images/bcd2085d42af2defbc25fb53a660ed1cb684d1c0547b52c0a80b016758bc0412.png)  
![图 5](images/120b2538ccca312cef5a6f699b950a09bf33f42230b79c4d2af94d889ddb6c51.png)  

点击install，安装完成后，点击“Restart IDE”，重新进入开始页面，选择New Project，再输入project名称，选择scala，Build System选择IntelliJ
![图 6](images/c374a9f158e541e592d516a7a7e8812a0486ed077e9281fbd80620be9e0a6692.png)  
点击Scala SDK右侧的Create：
![图 7](images/19e7ec2cbf7b7beffa0c0cd2d101a9144dcb451b0433efeff04c6057db1dc569.png)  
系统会根据我们.bashrc配置的SCALA_HOME找到Scala SDK，选择即可。
然后点击页面下端的“Create”。
执行，如果运行正常，说明IDE配置没有问题。

### 4.3 scala语言入门 <a id ="4.3"></a>
#### 4.3.1 基本数据类型 
基本数据类型有8种，但是没有string 但是scala基本数据类型包和java.lang是默认一起导入的。String在包中，也就是说java中有的数据类型都会有。
#### 4.3.2 变量声明 
格式： val(var)变量名：数据类型=初始值（必须有初值，除抽象类型）
val：不可变的，类似于java的final或c的const
var：可变的
数据类型可以不写，系统会自动判断。
数据类型还可以通过后缀来指定。
#### 4.3.3 算术操作符 
和java里的差不多，但和Python有区别
#### 4.3.4 关系运算 
与其他语言相同
#### 4.3.5 逻辑运算符 
与其他语言功能相同，写法上和c一样
#### 4.3.6 位运算符 
运算结果是整数，按二进制位进行对应计算
#### 4.3.7 赋值运算符 
与其他语言功能相同
#### 4.3.8 返回值 
对于一个语句块，最后一句的值就是本语句块的返回值，有时候可以使用return，但是不建议。
#### 4.3.9 if 
写法：
方式1：if(布尔表达式) x else y
方式2：if(布尔表达式) {
        语句块1;
    }
    else{
        语句块2;
    }
方式3：if(布尔表达式) {
        语句块1;
    }
    else if{
        语句块2;
    }
    else{
        语句块3;
    }

练习：
![图 8](images/32c07e66d9b547434c9ccb35a9d7f648b06398927c87094e1d484f3b90e93ede.png)  
#### 4.3.10 for 
for 语法：
for(变量<-序列){
    循环体
}
构建简单循环体，其他语言用range函数实现，但是scala用to（包含两个端点） 或 until（不包含右端点）。
练习：
![图 9](images/1bfda9f6521c33be6178f3289d868e84dd9fb836282b21653870c332a30964d0.png)  
![图 10](images/c5e983988914bbd28cfbd5a5fb5700a8f68d1060d1c5ece7775b744b2110f092.png)  

for中使用守卫（if），例如对于
```java
    for(i<-1 until 10)
    print(i+" ")
```
for循环中，只要偶数，写法为：
```java
    for(i<-1 until 10 if i%2 ==0)
    print(i+" ")
```
或者
```java
    for(i<-1 until 10 if(i%2==0))
    print(i+" ")
```
![图 11](images/e8a36f317d3830df9c2633f9ec8d9d9753586ac990901d276e51dd498c3f0298.png)  
for中的守卫可以写多个，例如如上程序，序列中剔除6
```java
    for(i<-1 until 10 if i%2 == 0 && i != 6)
    print(i+" ")
```
scala的for语句原生没有continue和break，如果一定要用，需要导入包。
#### 4.3.11 while 
与其他语言没有区别
#### 4.3.12 数组Array 
scala默认是长度不可变数组。如果要使用可变数组，需要导入包。
写法：
```java
val A=Array(1,2,3,4)
```
#### 4.3.13 映射Map 
就是Python中的字典，只是写法不同。
定义映射：
```java
val mapCase=Map("China"->"Beijing","age"->1500.123->456)
```
#### 4.3.14 元祖Tuple 
写法与其他语言相同。
访问方式：
使用 ._序号，**注意：序号从1开始**
例如：
![图 1](images/fc82c91bec61cd489c68700212273fb399b6ccc4d54c806881dfe6b785a8d6c9.png)  
#### 4.3.15 列表List 
与Array类似，区别是List是一个链表
#### 4.3.16 集合Set 
与Python基本相同

### 4.4 scala语言的函数式编程 <a id ="4.4"></a>
#### 4.4.1 普通函数
基本定义：
def 函数名(形参表（形参名：形参类型）):返回值={
    函数体（执行的最后一句是返回值，一般不写return）
}
返回值一般不写，系统会根据函数体执行自动识别。

匿名定义：
(参数表)=>{函数体}
当不作为函数的位置参数时，需要通过赋值进行调用。
例如：
```java
object Main {
  def main(args: Array[String]): Unit = {
    def myFun(a: Int): Float = {
      a.toFloat / 2
      //println(a)
    }
    val b=(x:Int,y:Int)=>{(x+y)/3}
    println(b(1,23))
  }
}
```
通配符（占位符）“\_”的使用：目的是简化函数写法。如果函数体中只使用一次，且逻辑上不会引起混淆的情况下。
使用“\_”，就没有必要写形参表了，直接写函数体即可，需要显示地指定数据类型。
例如：
```java
    val fun_1=(_:Int)+(_:Int)
    println(fun_1(1,2))
```
当函数作为参数时，由于函数在定义时已经指定了形参数据类型和返回值类型，所以可以省去类型指定，例如：
```java
    def myFun(fun:(Int,Int)=>Int,a: Int,b:Int,c:Int): Float = {
      fun(a,b)/c
    }
    println(myFun((_+_),1,23,3))
```
#### 4.4.2 高阶函数
##### （1）函数的参数是函数
```java
  def main(args: Array[String]): Unit = {
    val b=(x:Int,y:Int)=>{(x+y)/3}
    def myFun(fun:(Int,Int)=>Int,a: Int,b:Int,c:Int): Float = {
      fun(a,b)/c
    }

    val res=myFun(b,1,23,3)
    println(res)
  }
```
匿名函数作为函数的实参

```java
  def main(args: Array[String]): Unit = {
    //val b=(x:Int,y:Int)=>{(x+y)/3}
    def myFun(fun:(Int,Int)=>Int,a: Int,b:Int,c:Int): Float = {
      fun(a,b)/c
    }
    println(myFun((x:Int,y:Int)=>(x+y)/3,1,23,3))
  }
```
##### (2)函数的返回值是函数
定义：
def 函数名（形参表）：返回值类型是函数类型，例如：
```java
    def myfun2(a:Int):(Int,Int)=>Int={
      return(_+_)
    }
```
调用有两种形式：
（1）分步调用：第一步先得到返回的函数，第二步将参数传递到返回的函数中
```java
    def myfun2(a:Int):(Int,Int)=>Int={
      if(a>0) return(_+_)
      else return (_*_)
    }
    val fun=myfun2(3)
    println(fun(4,5))
    val fun2 = myfun2(-1)
    println(fun2(4, 5))
```
（2）一次调用完成
```java
def myfun2(a:Int):(Int,Int)=>Int={
      if(a>0) return(_+_)
      else return (_*_)
    }
    println(myfun2(3)(1,2))
```

## 五、spark编程 <a id ="5"></a>
>spark是一个编程框架。scala是一个编程语言。什么是框架呢？简单理解就是已经为你提供了很多类似函数的东西（我们称为算子），直接调用就很方便。
开发spark程序流程：spark使用本地模式（local），进行程序调试，完成后，生成jar包，上传到hadoop yarn集群上运行。

### 5.1 启动spark的scala环境 <a id ="5.1"></a>
安装spark环境：下载、解压、配置路径。
解压缩：tar -zxvf spark-3.2.4-bin-hadoop3.2.tgz 
测试：进入spark目录/bin
![图 3](images/c4b455dbd1a49e0e75d41fbd8e5489b73b90e88ba158164a4bcd531e9f5d8a65.png)  

配置：
* 赋值SPARK_HOME
* 将spark命令的路径配置到PATH
```shell
export SPARK_HOME=/home/bigdata3/software/spark-3.2.4

export PATH=$SPARK_HOME/bin:$PATH
```
保存退出。
测试：新启动一个终端，直接输入spark
![图 4](images/1898eee1bb6f0f0fa110c943ba7b640044411a75f3297fd88771af1073faebdd.png)  

### 5.2 RDD相关概念 <a id ="5.2"></a>
RDD是一个弹性分布式数据集。在集群的不同节点上进行计算，可以理解为是一个数据分布在各个不同节点上的数据集合，当对它进行操作计算时，实际上是对各个节点上的数据进行计算。

在不指定local的数目的时候，分区数一般<=cpu核心数。分区的目的是并行计算。

#### 5.2.1 RDD的依赖：
窄依赖：计算数据流在同一个区
计算过程与其他区域无关。

宽依赖：计算数据流存在跨区问题
本步计算必须等待上一步所有分区全部计算完成。

shuffle灾难：数据分区时，数据量相差巨大，当需要执行宽依赖计算时，等待耗时非常长（主要集中在大数据块中），这是我们编程的时候需要避免的问题。

#### 5.2.2 RDD计算
按惰性分类：转换操作和执行操作
RDD计算的惰性：转换操作在没有遇到执行操作的时候不执行
RDD转换操作的连续性：每一步转换操作的结果都是RDD，所以一般采用链式写法，但是执行操作的结果不一定是RDD，所以一般情况下，链式写法的最后一句是执行操作。

### 5.3 spark程序运行模式 <a id ="5.3"></a>
#### 5.3.1 local模式
单机模式。一般是调试程序的时候用
#### 5.3.2 集群模式
（1）standalone：是使用spark自身进行资源管理的集群模式
（2）mesos：已经过时
（3）Yarn：我们自己搭建的集群就是用yarn进行资源管理的，所以可以直接使用。
（4）Kubernets（k8s）：快速发展中

### 5.4 spark集成开发环境的使用（idea） <a id ="5.4"></a>
File->New->Project
![图 1](images/8bb8e6a1416ed0a7baa105b7a9d8fdcd7c6c789fa2f8960f4c5b3e8275839caa.png)  
点击create，测试当前Scala项目
添加spark的库：
File->Project Structure：
![图 2](images/0cd15738708b57e340336b44d982596a8bf136aae39381a8e534cc73f907bc2b.png)  
点“+”
![图 3](images/930e1e38ffdf32b3ae9020c7220cf85387eae500416597f03c1f4415575a14ac.png)  
选“java”
![图 4](images/14f41cd13ae538f64887aa0f980b40cd95b75fb6bf0cb95c466a7a098f831d3b.png)  
找到spark软件包下的jar目录，将所有文件都选中
![图 5](images/c7506ef1caeb0225996a76b5a1effd2577e046022d92b0ee0429ea9d424db432.png)  
点击ok，等待Indexing library进度条走完
手工引入spark框架
```java
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[*]")
    val sc = new SparkContext(conf)
```
输入过程中idea会自动添加import：
import org.apache.spark.{SparkConf, SparkContext}
说明spark项目正常建立完成。

### 5.5 RDD编程 <a id ="5.5"></a>
#### 5.5.1 生成RDD（在spark-shell下）
##### （1）从程序中的数据生成
```java
val data = Array(1, 2, 3, 4, 5)
val distData = sc.parallelize(data)

```
![图 1](images/5ac293f5c70efd72d85f4935bd36584c48d191b9ea04e0b71437df2a04fe9052.png)  
![图 2](images/1fa42cbf7ed06963d6ec4e4610b22e3b72c16ddc0ca5618e0ff90135ae75b83c.png)  

##### （2）从持久化的数据（文件）生成
A) 从本地文件生成
```java
val distFile=sc.textFile("/home/bigdata3/b.txt")
```
![图 3](images/079a1b6d1b6f687664ae62763068e005952c52c1404a0cad4f6cc27f67842958.png)  

B) 从hdfs文件生成
```java
val distFile=sc.textFile("hdfs://172.21.17.103:9000/b.txt")
```
### 5.5.2 生成RDD（在idea下）
##### （1）从程序中的数据生成
程序1：
```java
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val data = Array(1, 2, 3, 4, 5)
    val distData = sc.parallelize(data)
    distData.collect.foreach(println)
    Thread.sleep(2*60*1000)
  }
```
程序2：sparkDAG写法
>DAG图：有向无环图。简单理解就是用一条线把所有的操作都穿起来。

```java
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[2]")
    val sc = new SparkContext(conf)

    sc.parallelize(Array(1, 2, 3, 4, 5)).collect.foreach(println)
    Thread.sleep(2*60*1000)
  }
```
##### （2）从持久化的数据（文件）生成
A) 从本地文件生成
```java
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[2]")
    val sc = new SparkContext(conf)
    sc.textFile("/home/bigdata3/b.txt").collect().foreach(println)

    Thread.sleep(2*60*1000)
  }
```
B) 从hdfs文件生成
```java
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[2]")
    val sc = new SparkContext(conf)
    sc.textFile("hdfs://172.21.17.103:9000/b.txt").collect().foreach(println)

    Thread.sleep(2*60*1000)
  }
```

### 5.6 RDD操作 <a id ="5.6"></a>
以word count为例。
#### 5.6.1 filter
过滤不满足参数的参数是fun，要求此fun的返回值是boolean类型。
例：
```java
    val fileData = sc.textFile("/home/bigdata3/a.txt")
    fileData.filter(x=>x.contains("Good")).collect().foreach(println)
```
![图 1](images/9b070f8828672b1e2108897c76d54f3d4cd06901e0c9310c09350265a82a16d5.png)  
#### 5.6.2 map
将RDD中的每个元素用map的参数fun进行处理，返回的还是一个RDD。
例：
元素+10
```java
    val fileData = sc.textFile("/home/bigdata3/a.txt")
    val afterMap=fileData.map(_+10)
    afterMap.collect().foreach(println)
```
![图 2](images/e6070df5ff59479d9f8e89444193ad9c914583ccb178dbd1a0ee500e2c738532.png)  
例2：将每个元素写成（元素，1）形式
```java
    val fileData = sc.textFile("/home/bigdata3/a.txt")
    val afterMap=fileData.map((_,1))
      afterMap.collect().foreach(println)
```
![图 3](images/72c104ea48434791c91b25c5a1328bee2e71564a6023af38240d7eeb0327e69c.png)  

#### 5.6.3 flatMap
拍扁压平。
例1：（用flatMap读取文章中的每一个词）
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
    val afterFlatMap=fileData.flatMap(x=>x.split(" "))
      afterFlatMap.collect().foreach(println)
```
![图 4](images/c0653b35a3f8087615232e651d6d1152582d475162cea34098f0b260f1d8057d.png)  
例2：将文章中的每一个单词写成（单词，1）的形式
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
    val afterFlatMap=fileData.flatMap(x=>x.split(" "))
      afterFlatMap.collect()
    val mk = afterFlatMap.map((_,1))
    mk.collect().foreach(println)
```
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
    val afterFlatMap=fileData.flatMap(_.split(" ")).map((_,1))
    afterFlatMap.collect().foreach(println)
```
![图 5](images/6dbbedced1aefbaaab16bab9fe5426e2990a2d192ba8e19275919faac817d080.png)  
#### 5.6.4 reduceByKey
按照参数fun进行聚合。

例1：统计单词的个数
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
    val afterFlatMap=fileData.flatMap(_.split(" "))
      afterFlatMap.collect()
    val mk = afterFlatMap.map((_,1))
    mk.collect().foreach(println)

    val afterRed = mk.reduceByKey((x,y)=>x+y)
    afterRed.collect().foreach(println)
```
![图 6](images/fd4ef7d6258e1869e457402bbbd9c1c93951af3ee24cdc4d180c05b163404ed7.png)  
#### 5.6.5 排序
##### （1）用sortByKey
排序之前需要将原来的（单词，出现次数）变为（出现次数，单词）。因为sortBykey是按照第一个元素进行排序的。
```java
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[2]")
    val sc = new SparkContext(conf)
    val fileData = sc.textFile("/home/bigdata3/d.txt")
    val afterFlatMap=fileData.flatMap(_.split(" "))
      afterFlatMap.collect()
    val mk = afterFlatMap.map((_,1))
    mk.collect().foreach(println)

    val afterRed = mk.reduceByKey((_+_))
    val count = afterRed.map(tuple2=>(tuple2._2,tuple2._1)).sortByKey(ascending = false).map(x=>(x._1,x._2))
    count.collect().foreach(println)
```
![图 1](images/4390ce2c1bd72dbe7c820cc1165eadf558ccf2322c5446ff1b0aa0740205a5d2.png)  
改写成DAG写法
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey((x,y)=>(x+y))
      .map(tuple2=>(tuple2._2,tuple2._1))
      .sortByKey(ascending = false)
      .map(x=>(x._1,x._2))
      .collect().foreach(println)
```
![图 2](images/e4485ade40cf455839af03dc7e63970fddca82186e749893c2164fbb7bd7cc92.png)  

##### （2）用sortBy
sortBy：按指定数据排序，指定数据用参数fun来完成。形式：
sortBy[k](f: (T) ⇒ K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false)
      .collect().foreach(println)
```
#### 5.6.6 排序后取数据量最大的n个显示
例如：取出数据量最大的10个单词：take(10)
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false)
      .take(10).foreach(println)
```
#### 5.6.7 运算结果的持久化（用文件保存）
选用saveAsTextFile，参数：保存路径，要求是之前不存在，否则会报错：
![图 3](images/e2d0cc9008d1589f6e5553cb46d196db9ad55411e31f76e1316cda84310264de.png)  
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false)
      .saveAsTextFile("/home/bigdata3/wordCountOut/out1")
```
![图 4](images/a1a9fc94f0e735cc64473a7bc796d7f6a9203cdf3136f945968eee380be38e84.png)  
保存的文件的数量与分区有关。在5.7中，我们将具体解决这个问题。

### 5.7 分区 <a id ="5.7"></a>
分区的目的：并行计算
创建RDD时就进行分区，分区的默认策略时数量平均。
#### 5.7.1 创建RDD时的分区
观察分区数，分区内容。
创建RDD使用textFile时，第二个参数是指定分区数，分区时按原始数据顺序，基本上是数量平均。
程序：
```java
    val conf = new SparkConf().setAppName("testPartition").setMaster("local[*]")
    val sc = new SparkContext(conf)
    val myRdd = sc.textFile("/home/bigdata3/testPar.txt",minPartitions = 4)
    myRdd.saveAsTextFile("/home/bigdata3/testPartOut/2")

    sc.stop()
```

DAG:
![图 2](images/8febd8f08401a0db45b9386009a2cd2cd09b85af5910947bf7da5adce2dc5871.png)  

程序执行结果：
![图 1](images/1bde0b40a2715b19c660ab25f4c55d176d2059d8e7a27e469f7ca61edb8eae87.png)  

![图 5](images/89d2b58397584680e676cb2a559eeabd102093a2abd31fefeb8be95658a54d3e.png)  

![图 6](images/3bf5d8d2efc57e35e8be48ef4785a6404077097c56af8e35e495162ca86a2404.png)  

![图 7](images/2a30da70a870e9c97b15f442f5e7f6930d99a5d5f6a7f0df4ce990d0d88ef905.png)  

![图 8](images/584770c7968962771ac7696a74c98d801153ba530046f8c353d13eceaff2c283.png)  

#### 5.7.2 窄依赖
计算过程中，数据没有跨区操作，以map为例
程序：
```java
    val myRdd = sc.textFile("/home/bigdata3/testPar.txt",minPartitions = 4)
    myRdd.saveAsTextFile("/home/bigdata3/testPartOut/10")
    val afterMap = myRdd.map((_,1))
    afterMap.saveAsTextFile("/home/bigdata3/testPartOut/20")
```
由于有两个action：saveAsTextFile，所以有两个job：job0和job1。查看DAG：
![图 9](images/6780a41502101bd93d189fce284a44aa66c8fcbf341d1841327cf5f93c5f80d8.png)  
![图 10](images/a5c67d51f7669ee1c2da1f9b214ee76ac1da02df04ffb341b16945ad8247b6c1.png)  
![图 11](images/b074dee64bf4697cebbbab2be9b05158204251347f234bad25d1a4b7d2150476.png)  
查看数据，以第一个分区为例：
![图 12](images/1b01ecfe3e7122d0e978eb184914db191003d1aa5a777060853eeb39e77f41c2.png)  
![图 13](images/704f38ef15c60ea64c4001861bc4261fb32728eccc66a3e9d7d5d70ded6e8f52.png)  

#### 5.7.3 宽依赖
计算过程中，数据有跨区操作（shuffle）。以reduceByKey为例：
```java
    val myRdd = sc.textFile("/home/bigdata3/testPar.txt",minPartitions = 4)
    myRdd.saveAsTextFile("/home/bigdata3/testPartOut/101")
    val afterMap = myRdd.map((_,1)).reduceByKey(_+_)
    afterMap.saveAsTextFile("/home/bigdata3/testPartOut/201")
```
观察stage数量：

![图 16](images/e5cd5e56d0744e965ca13c6719fc75295b6000fa6e6ec885dd5f5cb875b81262.png)  
![图 19](images/673e645de94186da43c7b53ce8e32d9ac262b3c962b589d4990f8ef508c2c669.png)  
由于reduceByKey是宽依赖，多了一个stage：job1包含了两个stage：stage1和stage2。由于tasks是包含在stage中的，而我们没有改变分区数，所以tasks多了一倍

job->stage->task
观察数据，以第一个分区为例：
![图 14](images/6f9b32ac7ee804ae186333e60143a50aeb0169d7ab0554605fa65e67d03a8ebe.png)  
![图 15](images/c292c4c41e3d363e6fabbd1e62c92fb90d26178b4a0e813d603eaf57267e66e9.png)  
reduceByKey后与之前的数据相比，同一个序号的数分区的数据有些包含、有些不包含，所以证明有分区之间的数据流动。
#### 5.7.4 重分区repartition(numPartitions)
repartition一定是有shuffle操作
验证：使用repartition(numPartitions)之前和之后分区相同，如果同一分区的数据不完全相同，则证明一定有shuffle。
```java
    val myRdd = sc.textFile("/home/bigdata3/testPar.txt",minPartitions = 4)
    myRdd.saveAsTextFile("/home/bigdata3/testPartOut/301")
    myRdd.repartition(4).saveAsTextFile("/home/bigdata3/testPartOut/302")
```
SparkUI：
![图 1](images/0bf1b1fd66f47b2d023cf44166daf854b7c8c76b02fcfbe8d7a70efc554481bc.png)  
job0有一个stage，4个分区
![图 3](images/bb54e5ae683ef6ac2261481a8443ef2d6a403cd6b54a9acfdf4a6ad007cb7f91.png)  

job1有两个stage，因为有一个shuffle操作，四个分区，task数量：2*4=8
![图 2](images/92b3a6f39469803cd8ddc7b940e2e469daffee8f4c675f1011cd2bac79a0d5a6.png)  

比较数据：以分区0为例
![图 4](images/83f5f35abae99bfce42e767dd791650dd447778ee10f45ab4fc5c5a3d73f8128.png)  
![图 5](images/5418bddc9744bd4004c9ae6d109f929c75c480065c8314df2f0f777441395f51.png)  
两个分区数据不完全相同，说明有shuffle操作

#### 5.7.5 重写WordCount程序，使输出数据在一个分区里
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false)
      .repartition(1)
      .saveAsTextFile("/home/bigdata3/wordCountOut/out3")
```
运行结果
![图 6](images/56f021ed1ae24680a7918e2c828fb6cc7ef779c5f0deba6833da083145612e45.png)  
![图 7](images/e6b297d96d46e3f59d685d8d6d5108c64209e8719726d997835a821faa64376b.png)  
观察SparkUI：
job有两个：job0和job1
![图 8](images/b24b360779fe73190332f5d1342f992d294d7d32506fab84b64f7423add59e2a.png)  
job0的DAG，有两个stage，由reduceByKey划分:
![图 9](images/150e68d5b73096a54e616f0a44bced5df03f43986406a2975bf90c3d2a3557bf.png)  
job1:有4个stage，分别由reduceByKey、sortBy、repartition划分，由于stage2与job0的重合，没有进行真正的计算，所以skip
![图 10](images/7b61ab8b4f2cdaac40f9c8b15118d2db5dc2b5517511a2618db6f684ba67d95f.png)  

#### 5.7.6 RDD操作中的分区参数
如果一个操作没有分区参数，一般情况下是窄依赖，否则就是宽依赖。
可以通过使用分区参数避免repartition操作
例：通过使用sortBy的分区参数，避免使用repartition操作。sortBy的定义：
```java
def sortBy[K](f: (T) ⇒ K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)(implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]
Return this RDD sorted by the given key function.
```
修改后的程序：
```java
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false,numPartitions = 1)
      //可以按位置参数调佣调用写成sortBy(_._2,ascending = false,1)
      .saveAsTextFile("/home/bigdata3/wordCountOut/out3")
```
观察SparkUI：只有一个job：
![图 11](images/bac6f3a1d4e2173b3b0b6ace3798dbef4cd694145ff00cbc89ecad55f313ab20.png)  
有3个stage，由reduceByKey和sortBy进行划分
![图 12](images/caa58f0bac82624ace42352d3930baa29d1677792630cfb143d059594d80067d.png)  

### 5.8 生成jar包送到集群运行 <a id ="5.8"></a>

我们最终是要把程序提交到集群上进行运算的。所以，在这里我们将讲解如何将程序提交到集群进行运算。
#### 5.8.1 spark-submit使用方式
向集群提交运算程序的命令。在终端窗口输入：spark-submit，查看帮助。
命令用法：Usage: spark-submit [options] <app jar | python file | R file> [app arguments]
主要的options：
##### 1）--master 
&emsp;&emsp;MASTER_URL：程序运行在哪里，可以运行的方式：spark://host:port, yarn,k8s://https://host:port, or local (Default: local[*])
我们是要用yarn：--master yarn
##### 2）--deploy-mode 
&emsp;&emsp;根据Driver运行在哪里，有两种模式：
* client：driver运行在本机，由于本机需要与集群交互，所以需要ssh免密配置。
* cluster：driver运行在集群，本机只是将app传送到集群，相对client模式：看到的运行信息较少。
##### 3）--class 
&emsp;&emsp;app的主class，对于scala而言，是主object的名字。如果程序简单，可以不写。


#### 5.8.2 spark程序打成jar包
##### （1）idea打包前配置
File->Project Structure：
![图 1](images/a870f019306cd93c52ef82feb00a92533fd70f16c5e09c3d3cf3728d4fc4f5ec.png)  
点"+"号，按图示选择
![图 2](images/d60cec2d8cc213c609bf3f5e2ecfad4328adf7df4e1ee973f7780595104a7083.png)  
![图 3](images/48b5b7cb4254d3f18e8948b8f02a3febc77fec279a3ee19d0a93cd49164c4687.png)  
上图中Main Class选项，通过点击右侧的browse按钮选择：
![图 4](images/8cf3a5d3f1a50adf2557a455b41d3c68c9d2eaab0203730ef5a6d5b568e5433f.png)  
![图 5](images/2503012c701749d24d7508fb1142527b1814da13177b09d51b6a038b1bc561c9.png)  
观察jar包所在目录。点击“OK”：
![图 6](images/08096234bc7e2c9f4a547c908d84a120cb2365b707ef4c387ffef1b625324c2e.png)  
进行jar包的瘦身：把不需要的lib删除。把下图中的jar库除了最后一项，其他全部删除
![图 7](images/c6b492128441ef4269e90ce14e33f7dd380c878a1f15fd575f99f9ae7a9581d9.png)  
![图 8](images/a9a1e59285ae44015604fcab0fc94064b8978da845d9ab5a1a2470ce5a779770.png)  
观察界面上的Name和Output directory：分别是jar包的名字和生成jar包所在的目录。
点击“OK”，完成配置。

##### （2）程序打包
 （1）修改程序
修改目的，适应部署的情况
 （2）打包
程序当前部署到local模式，输入输出全部使用本地文件，部署时不需要传递Main参数。
```java
import org.apache.spark.{SparkConf, SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1").setMaster("local[*]")
    val sc = new SparkContext(conf)
    val fileData = sc.textFile("/home/bigdata3/d.txt")
      .flatMap(_.split(" "))
      .map((_,1))
      .reduceByKey(_+_)
      .sortBy(_._2,ascending = false,1)
      .saveAsTextFile("/home/bigdata3/wordCountOut/out6")
    sc.stop()
  }
}
```
打包：菜单Build-Build Artifacts：
![图 9](images/07786e25557b90fb89636625357776ba5d3e2a6fd7edb92eeac4e4e0b1788d66.png)  
第一次可以选择Build，以后选Rebuild。等进度条完成后，到生成jar的目录查看：
![图 10](images/5a164eadfb2b16cdd187478d7871d3ad898770cda73d1a832a04eb4dc4dc7055.png)  

#### 5.8.3 spark-submit部署程序
##### 1）local模式
（1）输入输出全部使用本地文件，部署时不需要传递Main参数。
部署命令：spark-submit app名字
![图 11](images/f854fe8136bfe171ac2f8516089523e59cc9cb4b65389c562865e67c7ef36199.png)  
![图 12](images/7640da41ea632f024c9cc3aad454a590fd7a42f7d4718e2743d43daeedb10dfa.png)  
![图 13](images/b53523e9d89a34039ca2c0aba334753128cd63684b89fc6ffb38a1d3a5259e09.png)  
（2）输入输出全部使用本地文件，部署时不需要传递Main参数,设置local分区数为4。
部署命令：spark-submit --master local[4] testSpark.jar 
![图 14](images/efb6443df5cbeef5179dbc5a3babde20ee98336bf45d770742e1660d8ddd2e08.png)  
（3）输入输出全部使用本地文件，部署时需要传递Main的实参,设置local分区数为4。
修改程序，将main的形参在程序中使用，将输入、输出用形参传递到程序中。
```java
object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("testSpark1")//.setMaster("local[*]")
    val sc = new SparkContext(conf)
    val fileData = sc.textFile(args(0))
      .flatMap(_.split(" "))
      .map((_,1)).reduceByKey(_+_)
      .sortBy(_._2,ascending = false,1)
      .saveAsTextFile(args(1))
    sc.stop()
  }
}
```
打包，部署：（要先删除wordCountOut目录）
spark-submit --master local[4] testSpark.jar "/home/bigdata3/d.txt" "/home/bigdata3/wordCountOut"

![图 15](images/7c6a24cfc053f59747ec6eec05ec53238032cfdf3c5074a1e5d701a62f6ac9fc.png)  
![图 16](images/52e772e665c72e4afb601298e84a1ccd58c7ba4fd8cf12f9e7c26b48e15cbfa6.png)  
（4）输入输出全部使用hdfs文件，部署时需要传递Main的实参,设置local分区数为4。
程序与上一步相同，只是传递不同的参数。
运行主机在集群外：hadoop fs -put d.txt hdfs://172.21.17.103:9000/wordCountIn001.txt
运行主机在集群内：hadoop fs -put d.txt /wordCountIn001.txt
spark-submit --master local[4] testSpark.jar hdfs://172.21.17.103:9000/wordCountIn001.txt hdfs://172.21.17.103:9000/wordCountOut001
![图 1](images/9890454e085956721bb73283bc278015c4632034b4767a5a566e72e919776dbc.png)  
![图 2](images/9a47b51f9d483427f9bb36772d05394c2142864f96d321fb38110b57bc34d7a3.png)  
![图 3](images/94848997313fcb68392f5a31bae2e2b591243851aec085b369b8a480d4dd9578.png)  

textFile输入是目录：要求目录下是文本文件
spark-submit --master local[4] testSpark.jar hdfs://172.21.2.6:9000/wordCountIn hdfs://172.21.2.6:9000/210110900120
![图 4](images/91a93659c81cf32e5989fff508cf210cd80ca5e5e5544ad1d9ba720f4e517cd2.png)  
![图 5](images/76dd96f97c37e1c100d2d468a487c5088ee4cf6260d424bdedf3f3c3ff6981dd.png)  
![图 6](images/381cc90ff65877b5eb3bd66d4826f7b6c767b8f8d582e00c8356295b65023ed9.png)  

##### 2）集群模式
（1）在yarn上运行spark程序的配置
在spark软件中配置，在conf目录下。配置spark-env.sh
```shell
export HADOOP_CONF_DIR=/home/bigdata3/software/hadoop-3.2.2/etc/hadoop
export YARN_CONF_DIR=/home/bigdata3/software/hadoop-3.2.2/etc/hadoop
```
如果当前主机在集群外，需要配置hosts文件，将集群的master节点写入。
如果当前主机在集群外，client模式需要ssh免密，否则只能是cluster模式。

 （2）在yarn上运行spark程序

* (A)cluster模式
使用上一步生成jar包。命令：
spark-submit --master yarn --deploy-mode cluster testSpark.jar hdfs://172.21.2.6:9000/wordCountIn hdfs://172.21.2.6:9000/210110900120x
![图 8](images/35ac2edade3fe58be1c746dda6ef36aedc0171600cb3f869cd10c263c08ed40a.png)  

![图 7](images/8ae7ea648e0e071aba1d3c907ec8bd91e40870823c7a5d2bae4689fb9d2621fa.png)  
* (B)client模式
spark-submit --master yarn --deploy-mode client testSpark.jar hdfs://172.21.2.6:9000/wordCountIn hdfs://172.21.2.6:9000/210110900120x
会出错，因为没有配置免密服务。需要先配置后才能使用。

## 六、电影数据实例分析 <a id ="6"></a>
### 6.1 数据集的数据格式 <a id ="6.1"></a>
评分文件是ratings.dat，格式是
UserID::MovieID::Rating::Timestamp
用户信息是users.dat，格式是
UserID::Gender::Age::Occupation::Zip-code
电影信息文件是movies.dat，格式是
MovieID::Title::Genres
### 6.2 找出评分排名前十的电影名 <a id ="6.2"></a>
思路：读入数据到RDD，计算电影评分的总分和评分次数，计算出平均评分，加入电影名，排序。
#### 6.2.1 读入数据到RDD
（1）读取ratings.dat
```java
    val ratingsRdd = sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
    ratingsRdd.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/2")
```
结果
![图 1](images/81f0fd712f4bbd4efb57aa33dba88a820c76d881d0383e67940fc0b2b0365c87.png)

（2）读取movies.dat
```java
    val moviesRdd = sc.textFile("/home/bigdata3/ml-1m/movies.dat")
    moviesRdd.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/movies/1")
```
结果：
![图 2](images/ed9d6e5dcbe1de0f8106944a5a097ac07c3e77891721e8f61d50a40fc215a8bd.png)  
#### 6.2.2 计算评分的总分
（1）将数据提取出来放到（电影id，评分）
```java
val ratingAfterMap = ratingsRdd.map(x=>x.split("::")).map(x=>(x(1),x(2)))
```
结果
![图 3](images/412dc835fe64a95d376ab512c8db0f44ba5525fe62370c987bae69e6cdac3998.png) 

说明数据类型错误，修改为：
```java
    val ratingAfterMap = ratingsRdd.map(x=>x.split("::")).map(x=>(x(1),x(2).toFloat)).reduceByKey(_+_)
    ratingAfterMap.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/7")
```
结果：
![图 1](images/446afc96a97bbf162ec7ea0d59aa026498a6ba1bfb9c590fe0f4a3468512a937.png)  

#### 6.2.3 计算每个电影评分的次数
参考WordCount程序
```java
val ratingCnt = ratingsRdd.map(x=>x.split("::")).map(x=>(x(1),1.toFloat)).reduceByKey(_+_)
 ratingCnt.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/7")
```
![图 2](images/14a329ee57c00558a660b5238399ed0429652f02daafd93e178295c2eda7a01e.png)  
#### 6.2.4 计算每个电影的评分平均值
（1）将每个电影的总分与评分次数对应存储
```java
    val ratingsAfterJoin = ratingAfterMap.join(ratingCnt,1)
    ratingsAfterJoin.saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/8")
```
结果：
![图 3](images/c7e8d11d3700cff988fea98eea107fcc47269e0dd6d0159b5d2e327456338d7f.png)  
（2）计算平均值
```java
    val ratingsAvg= ratingsAfterJoin.map(x=>(x._1,x._2._1/x._2._2))
    ratingsAvg.saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/9")
```
![图 4](images/ec46c9a3a2f3dcf95d88b89b7b37d11e73aed3d8288e5d9132547be8266b9b79.png)  
（3）按平均值排序
```java
    val ratingsSort =ratingsAfterJoin.sortBy(_._2,ascending = false,1)
    ratingsSort.saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/10")
```
当前的数据结构是：（电影id，平均分）
![图 6](images/43f2c7585cc1b32800ebca1815c69a37c88b1c7265ba65075560ae701a05dacc.png)  

#### 6.2.5 找出电影名
（1）movies.dat包括三个数据，我们需要movieid，title
```java
val moviesAfterMap = moviesRdd.map(x=>x.split("::")).map(x=>(x(0),x(1)))
moviesAfterMap.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/movies/4")
```
结果：
![图 7](images/db71e8d8da11b0122122f0b97fe66bab1cd8f17f9c2bdbdd020751ac42e52cb3.png)  
（2）合并
```java
    val ratingMovies = ratingsSort.join(moviesAfterMap,1)
    ratingMovies.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/12")
```
![图 8](images/a2683525eea0a1e6bad9952c149000496afee75e7d9c1c687ae877b47108d4b0.png)  

sortBy排序的次序发生了混乱，因为join是宽依赖操作
顺序应该是先join后sortBy
当前数据结构是（电影id，（平均分，电影名））
```java
    val ratingMovies = ratingsSort.join(moviesAfterMap,1).sortBy(x=>x._2._1,ascending = false,1)
    ratingMovies.repartition(1).saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/13")
```
![图 9](images/07e99617503291850e306d759b7214436e29c4997a4ec64ddf371bbadec2fe3f.png)  
#### 6.2.6 查看当前DAG图
![图 1](images/6092b80878c8703b76e302a21160d07044e2da92c1a32098e25fc5064b886a48.png)  

#### 6.2.7 修改当前程序为DAG写法
在6.2.6中我们发现有非常多的跨区流动，因此我们需要改变一下写法。

DAG写法的目的：1.减少job的数量 2.减少stage数量
分析：从数据来源看，读取的是两个格式不同的文件，数据读取不能合并。DAG主流程选择ratings数据比较合适，因为大部分的处理都是针对ratings。以下两个流程需要进行合并：
>这里我当时产生了一个疑问，就是根据什么进行合并？然后问了pf老师，得到的答案是实际上都能合并…非常灵活。

```java
    val ratingAfterMap = ratingsRdd.map(x=>x.split("::")).map(x=>(x(1),x(2).toFloat)).reduceByKey(_+_)
    val ratingCnt = ratingsRdd.map(x=>x.split("::")).map(x=>(x(1),1.toFloat)).reduceByKey(_+_)
    val ratingsAfterJoin = ratingAfterMap.join(ratingCnt,1)
```

合并后程序：
```java
    sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(x => x.split("::")).map(x => (x(1), (x(2).toFloat,1.toFloat)))
      .reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
      .repartition(1)
      .saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/15")
```
结果：
![图 2](images/ff8559baf0ab3578f90bcc5f9dd739e6db2b4ae2ae50f23252c4fd40cae57cd8.png)  

全部修改后的程序：
```java
import org.apache.spark.{SparkConf, SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("filmAnalysisDAG").setMaster("local[*]")
    val sc = new SparkContext(conf)
    val moviesRddIDAndTitle = sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(x=>x.split("::"))
      .map(x=>(x(0),x(1)))
    sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(x => x.split("::")).map(x => (x(1), (x(2).toFloat,1.toFloat)))
      .reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
      .map(x=>(x._1,x._2._1/x._2._2))
      .join(moviesRddIDAndTitle)
      .sortBy(x=>x._2._1,ascending = false,1)
      //对数据调整
      .map(x=>(x._2._2,x._2._1))
      .take(10).foreach(println)
      
  }
}
```
结果：
![图 4](images/f7f6057d9ce2413929964ae26daafdf727586ff12b3ac24e45a4e754aa9a59ac.png)  

>这里很怪，按道理不应该最高评分都是5.0，那就说明数据中存在问题，我们应该事先进行数据清洗。但是没关系，我们会在6.2.8中解决它。

查看DAG图：
![图 3](images/14c95004e1215332ab3bea7506fd9c75ac9815fbfdc440d284e0a3f293b8898c.png)  

相对于原来的结果stage由8个变成了5个，减少了很多。

#### 6.2.8 数据清洗
数据计算前，进行数据预处理，得到需要的数据，并剔除不完整的数据。
数据计算进行过程中，根据计算中间结果，抛弃不符合要求的数据。

（1）数据预处理
剔除ratings中不完整的数据：数据长度不对：正常是一个记录有4个数据；数据类型不对：比如评分字面上是字符。
以长度不对为例：
因为给的数据集都是完整的，只能手动构建一个拥有长度不完全正确的数据集。用filter操作，程序：
```java
    sc.textFile("/home/bigdata3/myRatings.dat")
      .map(x => x.split("::"))
      .filter(x=>x.length==4)
      //.foreach(x=>x.foreach(println))
      .repartition(1)
      .saveAsTextFile("/home/bigdata3/filmAnalysisOut/ratings/21")
```
filter(x=>x.length==4)是只保留长度为4的数据。

（2）计算过程中的数据清洗
目的是抛弃评分次数较少的数据，例如少于50次的，.filter(x=>x._2._2>=50)

完整程序：
```java
import org.apache.spark.{SparkConf, SparkContext}

object Main {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName("filmAnalysisFilter").setMaster("local[*]")
    val sc = new SparkContext(conf)
    val moviesRddIDAndTitle = sc.textFile("/home/bigdata3/ml-1m/movies.dat")
      .map(x => x.split("::"))
      .map(x => (x(0), x(1)))
    sc.textFile("/home/bigdata3/ml-1m/ratings.dat")
      .map(x => x.split("::"))
      .filter(x=>x.length==4)
      .map(x => (x(1), (x(2).toFloat,1.toFloat)))
      .reduceByKey((x,y)=>(x._1+y._1,x._2+y._2))
      .filter(x=>x._2._2>=50)
      .map(x => (x._1, x._2._1 / x._2._2))
      .join(moviesRddIDAndTitle)
      .sortBy(x => x._2._1, ascending = false, 1)
      //对数据调整
      .map(x => (x._2._2, x._2._1))
      .take(10).foreach(println)
  }
}
```
运行结果：
![图 1](images/d4661b70c1f168a903d3a4af519d4c484fa7f5d6b4e3700a1c05e1a2d8f71283.png)  
最高分为4.6，比较符合正常的逻辑。
>ps.在这我当时也有一个疑问就是怎么知道我操作后得到的到底是tuple还是数组（这将决定我们获取元素时的写法）？问了一下pf老师（其实很早之前就想问了orz），如果我们用（，）这样写起来（比如map），那么返回的一定是tuple，否则，需要阅读官方文档得知操作后的数据类型。

## 七、后记 <a id ="7"></a>
~~如果你能读到这篇文章说明我终于克服了懒惰把它上传到了博客上orz…（本来是想传的但是好像读不了图片来着，待我考完试还记得的话研究一下）~~
非常感谢每一个能阅读到这里的大家！！也非常开心大家和我一样对这个东西抱有兴趣！这篇文章里涉及到的Hadoop、scala、spark等等很多很多概念都是我从来没有接触过的。刚开始的时候确实很痛苦啦…三天两头跑来机房配环境啥的，但是到后面发现，诶，好像是一件蛮酷的事情耶！在这里真的真的非常感谢pf老师呜呜呜，对我的每一个刁钻奇怪的问题都能一一接招，然后还能耐心教我怎么做，怎么解决（要不是pf老师这么耐心我估计早半途而废了orz希望别觉得我烦呜呜）。以及从pf老师身上真的学会了很多人生大道理和学习思想，对我的改观也很大呜呜
虽然折腾了一个学期但是总感觉自己连入门都算不上就是说…但是写到这里的时候我还有一天就要期末考了TAT，只好先搁置一段时间了。希望自己不要直接开摆…按照计划的话应该还会有后面关于sql和streaming的相关的内容总结…应该…会有…的吧？